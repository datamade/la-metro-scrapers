# This is a basic workflow to help you get started with Actions

name: Scrape

# Controls when the workflow will run
on:
  
  schedule:
    - cron:  '15 4 * * *'   
  # Allows you to run this workflow manually from the Actions tab
  workflow_dispatch:
    inputs:
      specific_scraper:
        type: choice
        description: Which scraper to run?
        options: 
        - bills
        - events
        - people
      window:
        description: How many days to scrape?
        type: string

concurrency:
  group: lametro-scraper
    
# A workflow run is made up of one or more jobs that can run sequentially or in parallel
jobs:
  # This workflow contains a single job called "build"
  scrape:
    # The type of runner that the job will run on
    runs-on: ubuntu-latest

    # Steps represent a sequence of tasks that will be executed as part of the job
    steps:
      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - uses: actions/checkout@v3
      - name: install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y libgdal-dev
          pip install --upgrade pip
          pip install -r requirements.txt
          
      - name: run scraper without window
        if: ${{ !inputs.window }}
        env:
          DJANGO_SETTINGS_MODULE: pupa.settings
          SENTRY_DSN: ${{ secrets.SENTRY_DSN }}
          DATABASE_URL: ${{ secrets.DB_CONNECTION_STRING }}
        run: pupa update lametro ${{ inputs.specific_scraper }} --rpm=0
        
      - name: run scraper with window
        if: ${{ inputs.window }}
        env:
          DJANGO_SETTINGS_MODULE: pupa.settings
          SENTRY_DSN: ${{ secrets.SENTRY_DSN }}
          DATABASE_URL: ${{ secrets.DB_CONNECTION_STRING }}
        run: pupa update lametro ${{ inputs.specific_scraper }} window=${{ inputs.window }} --rpm=0
        
      - name: update vote count
        env:
          DATABASE_URL: ${{ secrets.DB_CONNECTION_STRING }}
        run: psql $(echo $DATABASE_URL) -f scripts/vote_counts.sql
      - name: merge terms
        env:
          DATABASE_URL: ${{ secrets.DB_CONNECTION_STRING }}
        run: psql $(echo $DATABASE_URL) -f scripts/merge_memberships.sql
      - name: keepalive
        uses: gautamkrishnar/keepalive-workflow@v1
  index-and-stats:
    runs-on: ubuntu-latest
    needs: scrape
    steps:
      - name: update search index
        uses: michcio1234/heroku-run@0.1.1
        with:
          heroku_api_key: ${{ secrets.HEROKU_API_KEY }}
          heroku_email: ${{ secrets.HEROKU_ACCOUNT }}
          heroku_app_name: ${{ secrets.HEROKU_APP }}
          command: python manage.py update_index --batch-size=50 --age=1
      - name: update stats
        uses: michcio1234/heroku-run@0.1.1
        with:
          heroku_api_key: ${{ secrets.HEROKU_API_KEY }}
          heroku_email: ${{ secrets.HEROKU_ACCOUNT }}
          heroku_app_name: ${{ secrets.HEROKU_APP }}
          command: python manage.py populate_person_statistics
      - name: clear cache
        uses: michcio1234/heroku-run@0.1.1
        with:
          heroku_api_key: ${{ secrets.HEROKU_API_KEY }}
          heroku_email: ${{ secrets.HEROKU_ACCOUNT }}
          heroku_app_name: ${{ secrets.HEROKU_APP }}
          command: python manage.py clear_cache
